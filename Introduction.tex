\section{Introduction}
\label{sec:intro}
%
\cite{Wurm08}(KIVA) \cite{Eche08}(Logistics)

The increasing need for fast and flexible commissioning (i. e., order picking and collection of
unstructured goods from storage compartments in warehouses) in logistic scenarios has created
substantial interest for autonomous robotic solutions. This was also evidenced by a recent BBC
investigation into a UK-based Amazon warehouse, which highlighted that the dull and strenuous nature
of commissioning could cause mental and physical illness in human workers. Amazon themselves took
action by organizing their first Picking Challenge at ICRA 2015.

The proposed APPLE project addresses the following important sub-task chain which occurs during
commissioning in prototypical warehouses: picking of goods from a storage location, subsequent
placement on a standard EUR-pallet and transport of the filled pallet to a target location. The
process has to be carried out in a manner which is safe for humans operating in the same
environment.

The key obstacle for many application scenarios is the autonomous grasping in uncertain real-world
environments. Currently, despite of a large research effort, no commer- cially viable solution is
available for this problem. State of the art autonomous grasping systems~\cite{Bere07, Srin10,
  Krug14a} commonly employ sampling based planners~\cite{LaVa06} to generate online reach-to-grasp
motion plans for offline planned grasps which are stored in a database.  During the execution phase,
such approaches necessitate many futile motion planning attempts which often incurs significant time
delays mainly due to the frequent colli- sion checks which are necessary to avoid the robot coming
in contact with itself or the environment.  For APPLE, we adopted a real-time reactive control
approach for manipulator mo- tion generation which allows to exploit redundancy, opposed to the
commonly used sense-plan-act architectures which constrain all manipulator DoF. The main idea is to
formulate a hierarchical set of tasks~\cite{Sams91} such as “move end-effector on this plane” or
“avoid joint limits” and to compute controls such that tasks of lower priorities are exe- cuted in
the null-space of higher ranked tasks~\cite{Sici91, Sent10}.

The aim is to reduce the dependence on classical, sampling based motion planning and to move towards
reactive feedback control to generate and execute complex motion behaviors of a robot.  Here, only
high-level behavioral goals (e. g., “go to this region” or “stay above obstacle plane”) are
specified in form of task functions~\cite{Sams91}. An intelligent control algorithm, which is based
on embedded optimization of these task functions, then handles the details and synthesizes
appropriate motions automatically in an online fashion. Opposed to classical sense-plan-act
architectures, in this paradigm only task-relevant Degrees of Freedom (DoF) need to be constrained,
which allows to exploit kinematic redundancies, e. g., for a manipulator to avoid unexpected
obstacles. Regarding grasp planning, we follow the general tenet and will extract redundant
representations in form of constrained pose intervals instead of discrete poses
%
\begin{figure}[t!]
\begin{center}
\includegraphics[width =0.8\linewidth]{figs/apple_demonstrator}
%\vspace{-0.25cm}
\caption{\textit{The APPLE demonstrator:} A KUKA LBR iiwa arm (3) is mounted on a retrofitted Linde
  CitiTruck AGV (6). A Velodyne Lidar (4) is used for localization, human worker detection is
  carried out with the RefleX camera system (5). The depicted Velvet Fingers gripper (2) is a further
  developed and smaller version of the gripper used in the FP7 IP project RobLog. Each of the
  gripper’s two fingers has a planar RR manipulator structure with two rotary joints and active
  surfaces which are implemented by conveyor belts on the inside of the two phalanges. The
  mechanical structure of each finger is underactuated and comprises one actuated Degree of Freedom
  (DoF) for opening and closing and two DoF for the belt movements. If, during grasping, the
  proximal phalanges are blocked by an object, the gripper’s distal phalanges continue to “wrap
  around” and envelope it in a firm grasp. Object and pallet detection is done with an ASUS Xtion
  Pro camera (1) which is mounted on the gripper's palm.}
\label{fig:robot}
\vspace{-0.5cm}
\end{center}
\end{figure}
%